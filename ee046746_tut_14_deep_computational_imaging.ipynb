{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "#### Elias Nehme\n",
    "\n",
    "## Tutorial 14 - Deep Computational Imaging\n",
    "---\n",
    "<img src=\"./assets/tut_14_teaser.gif\" style=\"width:800px\">\n",
    "\n",
    "* <a href=\"https://www.nature.com/articles/s41377-020-00403-7\">Image source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [What is Computational Imaging?](#-What-is-Computational-Imaging?)\n",
    "* [Compressive Imaging](#-Compressive-Imaging)\n",
    "    * [Depth Encoding PSF](#-Depth-Encoding-PSF)\n",
    "* [Deep \"Optics\"](#-Deep-Optics)\n",
    "    * [Computer Vision Pipelines](#-Computer-Vision-Pipelines)\n",
    "    * [Differentiable Optics](#-Differentiable-Optics)\n",
    "* [Applications](#-Applications)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/64/000000/fantasy.png\" style=\"height:50px;display:inline\"> What is Computational Imaging?\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_ci_1.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/64/000000/fantasy.png\" style=\"height:50px;display:inline\"> What is Computational Imaging?\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_ci_2.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressive Imaging\n",
    "--- \n",
    "* Depth encoding PSF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Depth Encoding PSF\n",
    "--- \n",
    "\n",
    "* Measurement is a 2D image:\n",
    "\n",
    "<img src=\"./assets/tut14_cs_0.jpg\" width=\"400\">\n",
    "\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Image source - Optica 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Depth Encoding PSF\n",
    "--- \n",
    "\n",
    "* Recovery is a 3D volume:\n",
    "\n",
    "<img src=\"./assets/tut14_cs_1.gif\" width=\"400\">\n",
    "\n",
    "* What?? How?!\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Image source - Optica 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Depth Encoding PSF\n",
    "--- \n",
    "\n",
    "* Depth Encoding Impulse Response/Point Spread Function (PSF)\n",
    "    * Main idea is to encode depth in the shape generated on the 2D sensor\n",
    "    * This way, you can recover the depth from looking at the created shape\n",
    "\n",
    "<img src=\"./assets/tut14_cs_2.gif\" width=\"600\">\n",
    "\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Image source - Optica 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Depth Encoding PSF\n",
    "--- \n",
    "* Where is this inspired from?\n",
    "\n",
    "<img src=\"./assets/tut14_cs_11.jpg\" width=\"600\">\n",
    "\n",
    "* At sea, the light coming from above the water is going through different amount of water, since the surface of the water isn't still and smooth. \n",
    "* This means that each point of light that reached the bottom has gained a different amount of phase.\n",
    "  * It's a random amount phase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing\n",
    "--- \n",
    "\n",
    "* How to get a 3D reconstruction from a 2d image? formulate an optimization problem!\n",
    "* Writing down the problem in matrix formulation: \n",
    "\n",
    "<img src=\"./assets/tut14_cs_3.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing\n",
    "--- \n",
    "* $b$ - the column stack of the 2D measured image\n",
    "* $H$ - the convolution system that creates a 2D projection of a given 3D volume.\n",
    "  * This is built from the PSF\n",
    "* $v$ - the column stack of all the voxels in the 3D volume.\n",
    "  * This is the unknown we want to solve for\n",
    "* $e$ - noise added to each pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing\n",
    "--- \n",
    "\n",
    "* Compressive Imaging gives the solution by a \"MAP\" estimator under certain **conditions**\n",
    "\n",
    "<img src=\"./assets/tut14_cs_4.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing\n",
    "--- \n",
    "* Conditions:\n",
    "  1. The columns of $H$ have low correlation or no correlation at all\n",
    "  2. **Reularization condition**: $v$ or a transformation of $v$, $\\Psi v$ is a sparse signal (A small amount of coefficients are different from zero)\n",
    "* Under (1) + (2) the optimization probelm for $\\hat{v}_{TV}$ can be solved\n",
    "  * i.e. reconstructing the 3d volume ($v$) from a 2d image ($b$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing: Low Correlation Condition\n",
    "--- \n",
    "\n",
    "* Does the low-correlation condition on $H$ hold?\n",
    "\n",
    "<img src=\"./assets/tut14_cs_7.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing: Low Correlation Condition\n",
    "--- \n",
    "\n",
    "* Does the low-correlation condition hold?\n",
    "\n",
    "<img src=\"./assets/tut14_cs_8.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Images taken from Optica 2018</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing: Low Correlation Condition\n",
    "--- \n",
    "* Since the pattern is semi-random, moving it laterally (movement on axes $x$, $y$) will cause the autocorrelation to diminish\n",
    "  * The diminishing rate is very fast\n",
    "* Axial movement (on axis $z$, i.e. depth) will also cause the autocorrelation to diminish, since the random areas are widening/shrinking and don't overlap with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing: Low Correlation Condition\n",
    "--- \n",
    "\n",
    "* Does the low-correlation condition hold?\n",
    "\n",
    "<img src=\"./assets/tut14_cs_9.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Images taken from Optica 2018</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressed Sensing: Low Correlation Condition\n",
    "--- \n",
    "* If we take the pattern under axial/lateral shifts, and take each impulse response as a column in $H$, $H$ can be approximated as a random matrix and so the columns have low correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressive Sensing: Sparsifying Transform\n",
    "--- \n",
    "\n",
    "* Maybe $v$ isn't sparse?\n",
    "  * Apply $\\Psi$, a \"Sparsifying Transform\" to it.\n",
    "  * Now $\\Psi v$ is surely sparse, and the second condition holds.\n",
    "* \"Sparsifying Transform\" - Gradient, Wavelet, Total Variation, Learned Dictionary, etc.\n",
    "\n",
    "<img src=\"./assets/tut14_cs_6.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressive Imaging: Pipeline\n",
    "--- \n",
    "\n",
    "* Overall framework:\n",
    "\n",
    "<img src=\"./assets/tut14_cs_5.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://www.osapublishing.org/optica/fulltext.cfm?uri=optica-5-1-1&id=380297\">Image source - Optica 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/zip.png\" style=\"height:50px;display:inline\"> Compressive Imaging: Summary\n",
    "--- \n",
    "\n",
    "* Pros:\n",
    "    * Clear and nice theory developed over ~20 years (Prof. Michael Elad @ CS, Prof. Yonina Eldar @ Weizmann)\n",
    "    * Promising results in various fields: medical imaging, radar, signal processing, image processing, etc.\n",
    "\n",
    "* Cons:\n",
    "    * Hard to generalize to semantic tasks like classification/segmentation\n",
    "    * Performance is usually limited at low SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/wired/64/000000/switch-camera.png\" style=\"height:50px;display:inline\"> Deep Optics\n",
    "--- \n",
    "* Computer Vision Pipelines\n",
    "* Differentiable Optics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "* How do CV pipelines work?\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_2.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_3.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_4.jpg\" width=\"800\">\n",
    "\n",
    "* Take the bayer pattern (Each pixel only detects a single color), and apply demosaicing to get 3 colors per pixel\n",
    "* Filter out demosaicing/measurement noise\n",
    "* Filter out motion blur\n",
    "* etc...\n",
    "* We do all this to maximize the PSNR to get a clean and sharp images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_5.jpg\" width=\"800\">\n",
    "\n",
    "* How do we set the CNN? Training set of images and labels, and train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_7.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* Each part of this pipeline was designed seperately from the other\n",
    "  * Specifically - The first 2 parts are trying to create a clean and sharp image, without thinking about the network that comes afterwards. And vice versa - the CNN was designed with the knowledge that the dataset is composed of clean images.\n",
    "<img src=\"./assets/tut14_cv_pipe_8.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_9.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* Why is using the same \"vision system\" for different task is sub-optimal?\n",
    "  * Animal vision is adapted to the surrounding and the day-to-day \"task\"\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_10.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* In \"standard\" deep image processing, we have the task's loss, and we backpropagate to update the network's weights and improve its performance on the task.\n",
    "  * This is done based on clean and sharp images that we get from the normal ISP\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_11.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* Instead, in Deep Computational Imaging, we backpropegate those gradients up to the camera, and ensure the optics and the networks are optimized concurrently to improve the performance on the task.\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_12.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "* Why is this better than optimizing them seperately?\n",
    "  * Lets think about a system placed at the garden of a pet-daycare, and should open a different gate for the cats and the dogs.\n",
    "  * The camera placed there would only ever see cats and dogs.\n",
    "  * Using Deep Computation Imaging, the camera's optics would amplify the distinguishing features between cats and dogs\n",
    "    * Filming any other object will result in weird and undistinguishable images. But the camera should anyways never see any other object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* Image classification with specialized \"optics\"\n",
    "\n",
    "* At the start of the training loop, the classification will not be correct, as expected.\n",
    "  * The gradients will backpropegate through the network to the optics.\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_13.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "* Main idea: optimize the optics and the algorithm jointly to excel in the final task\n",
    "  * We'll get new cameras dedicated to the task\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_14.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-identification.png\" style=\"height:50px;display:inline\"> Computer Vision Pipelines\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_cv_pipe_15.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "* So far we drew an arrow in the diagram and said \"the gradients will backpropagate\"\n",
    "  * How do we actually do it?\n",
    "  * We model the camera as a **differentiable optical model**\n",
    "<img src=\"./assets/tut14_diff_opt_1.jpg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "* Image formation model\n",
    "  * A phase mask controls the impulse response (PSF) of a 3D point in the scene\n",
    "  * The 3D scene is convolved (3D convolution) with this PSF\n",
    "  * Noise is added to the output\n",
    "  * This is projected onto a 2D sensor to create a 2D image\n",
    "<img src=\"./assets/tut14_diff_opt_2.jpg\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* Now we know how to backpropegate up to the PSF (since we know a 3D convolution is differentiable, and an addition of noise is still differentiable)\n",
    "* But how do we map the optical element to the PSF?\n",
    "  * How do we backpropegate through a physical lens?\n",
    "<img src=\"./assets/tut14_diff_opt_3.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_diff_opt_4.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* Model the light coming from a point in the scene until it reaches the camera\n",
    "  * The light gains a quadratic phase as long as it travels through space\n",
    "<img src=\"./assets/tut14_diff_opt_5.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* When the light hits the phase mask, it gains a differnt amount of phase depending on where it hits the phase mask, since every point in the phase mask has a different depth.\n",
    "  * This phase mask will define an impulse response, based on the refractive index of the phase mask and the depth at each point\n",
    "<img src=\"./assets/tut14_diff_opt_6.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* The refractive index dictates a multlipicative scalar on the gained phase\n",
    "\n",
    "<img src=\"./assets/tut14_diff_opt_7.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* The different depth at each point, modeled as the **height profile** $\\Phi(x)$ of the phase mask, will of course also affect the phase gained by the light, and the autocorrelation of the light when it reaches the sensor\n",
    "<img src=\"./assets/tut14_diff_opt_8.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* How to model the height profile along a 2D space?\n",
    "  * We can model it per-pixel, i.e. each point in the phase mask\n",
    "  * Or we can create a spanning basis of the profile.\n",
    "* Generally: we have efficient ways to model it.\n",
    "<img src=\"./assets/tut14_diff_opt_9.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* After the light has passed through the phase mask, it continues to propegate through empty space (from the phase mask to the sensor)\n",
    "* This propagation is approximated by scalar optics with Fresnel's approximation\n",
    "\n",
    "<img src=\"./assets/tut14_diff_opt_11.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "* What we measure at the sensors is the intensity of the light that reached the sensors.\n",
    "  * The intensity is the squared norm of the field\n",
    "<img src=\"./assets/tut14_diff_opt_12.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "<img src=\"./assets/tut14_diff_opt_13.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* Finally: this modeling process gave us an impulse response dependant on the:\n",
    "  * unknowns:\n",
    "    * $z'$ - the depth of the point in the scene\n",
    "    * $k=\\frac{2 \\pi}{\\lambda}$ - the wavelength of the incoming light from that point\n",
    "  * known:\n",
    "    * $z$ - the distance of the sensor from the phase mask\n",
    "    * $\\Phi(x,y)$ - the phase mask we can change to dictate diffent phases gained based on depth\n",
    "* All of this is differentiable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* Now it's easy to train this with a network!\n",
    "  * We have an differentiable optic simulator\n",
    "  * We have a differentiable NN\n",
    "  * We can train end-to-end using simulations of 3D scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* Let us summerize with a classification task!\n",
    "<img src=\"./assets/tut14_diff_opt_14.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "\n",
    "* At test time, we throw away the simulator, create the phase mask, insert it into a real camera, and take pictures of a real 3D scene!\n",
    "  * Now we can finetune the NN with real physics.\n",
    "\n",
    "<img src=\"./assets/tut14_diff_opt_15.jpg\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/camera-addon.png\" style=\"height:50px;display:inline\"> Differentiable Optics\n",
    "---\n",
    "* For those that know it:\n",
    "  * You can think of the system as an encoder-decoder system:\n",
    "    * Where a 3D volume is inserted on one side, encoded into a 2D image with the camera\n",
    "    * The 2D image is the \"latent space\"\n",
    "    * Then the latent space is decoded to a 3D volume via a NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/wired/64/000000/untested.png\" style=\"height:50px;display:inline\"> Applications\n",
    "--- \n",
    "* Extended Depth of Field\n",
    "* Monocular Depth Estimation / Depth from Defocus\n",
    "* High Dynamic Range Imaging\n",
    "* Video Compressive Sensing\n",
    "* Computational Microscopy (Will not show examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/aperture.png\" style=\"height:50px;display:inline\"> Application 1: Extended Depth of Field (EDOF)\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_app_edof_0.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://dl.acm.org/doi/10.1145/3197517.3201333\">Image source - ACM SIGGRAPH 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/aperture.png\" style=\"height:50px;display:inline\"> Application 1: Extended Depth of Field (EDOF)\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_app_edof_1.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://dl.acm.org/doi/10.1145/3197517.3201333\">Image source - ACM SIGGRAPH 2018</a>\n",
    "\n",
    "\n",
    "\n",
    "* Both look blurry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/aperture.png\" style=\"height:50px;display:inline\"> Application 1: Extended Depth of Field (EDOF)\n",
    "--- \n",
    "\n",
    "* Remember the NN the deep computational camera must pass through!\n",
    "\n",
    "<img src=\"./assets/tut14_app_edof_2.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://dl.acm.org/doi/10.1145/3197517.3201333\">Image source - ACM SIGGRAPH 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/ios-glyphs/64/000000/abscissa.png\" style=\"height:50px;display:inline\"> Application 2: Monocular Depth Estimation\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_app_dfd_0.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf\">Image source - ICCV 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/ios-glyphs/64/000000/abscissa.png\" style=\"height:50px;display:inline\"> Application 2: Monocular Depth Estimation\n",
    "--- \n",
    "\n",
    "* The PSF is depth dependant.\n",
    "  * At training we have a depth map as well.\n",
    "  * Here the input is an image, and the depth map gives us the mask needed to simulate the depth affected PSF, to created the simulated 2D image.\n",
    "\n",
    "<img src=\"./assets/tut14_app_dfd_1.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf\">Image source - ICCV 2019</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./assets/input_vid_dfd.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"./assets/input_vid_dfd.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./assets/output_vid_dfd.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"./assets/output_vid_dfd.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./assets/bb_3d.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"./assets/bb_3d.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/officel/60/000000/width.png\" style=\"height:50px;display:inline\"> Application 3: High Dynamic Range Imaging\n",
    "--- \n",
    "\n",
    "* Usually the sensor model is cutting off the dynamic range, so when trying to photograph high-dynamic range we get saturation in different parts of the scene.\n",
    "* In HDR, we take 3 different photos with different dynamic ranges, and combine them to get a single high-dynamic range photo.\n",
    "* Can we optimize a phase mask such that the camera will create a single image from which we can get an HDR image?\n",
    "\n",
    "<img src=\"./assets/tut14_app_hdr_0.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf\">Image source - CVPR 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/officel/60/000000/width.png\" style=\"height:50px;display:inline\"> Application 3: High Dynamic Range Imaging\n",
    "--- \n",
    "\n",
    "* If the optics is creating copies of the scene at lowered intensity.\n",
    "  * Each point in the image is captured as 3 points, where the side lobes are at a lower intensity than the middle lobe.\n",
    "  * This means if the middle point is in saturation, there are still 2 copies of that point that aren't in saturation.\n",
    "\n",
    "<img src=\"./assets/tut14_app_hdr_1.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf\">Image source - CVPR 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/officel/60/000000/width.png\" style=\"height:50px;display:inline\"> Application 3: High Dynamic Range Imaging\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_app_hdr_2.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf\">Image source - CVPR 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/officel/60/000000/width.png\" style=\"height:50px;display:inline\"> Application 3: High Dynamic Range Imaging\n",
    "--- \n",
    "\n",
    "* After inserting the captured image to a NN we get an HDR reconstruction\n",
    "  * Notice that the normal NN trained on saturated images and HDR groud truth can't reconstruct well, since the data is missing from a normal image - and therefore it's hard for the network to \"understand\" what's going on there\n",
    "\n",
    "<img src=\"./assets/tut14_app_hdr_3.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf\">Image source - CVPR 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/material-outlined/48/000000/video-trimming.png\" style=\"height:50px;display:inline\"> Application 4: Video Compressive Sensing\n",
    "--- \n",
    "\n",
    "* Try to get from a single frame the sub-frames\n",
    "* Challenge is to learn the binary masks $\\Phi$ to recover video $x$ from snapshot $y$\n",
    "  * $\\Phi$ dictates which pixels are captured at each moment (sub-frame)\n",
    "\n",
    "<img src=\"./assets/tut14_app_vid_cs_0.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://www.sciencedirect.com/science/article/pii/S1051200419301459\">Image source - DSP Magazine 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/material-outlined/48/000000/video-trimming.png\" style=\"height:50px;display:inline\"> Application 4: Video Compressive Sensing\n",
    "--- \n",
    "\n",
    "<img src=\"./assets/tut14_app_vid_cs_1.jpg\" width=\"800\">\n",
    "\n",
    "* <a href=\"https://ieeexplore.ieee.org/document/9064896\">Image source - ICCP 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/material-outlined/48/000000/video-trimming.png\" style=\"height:50px;display:inline\"> Application 4: Video Compressive Sensing\n",
    "--- \n",
    "\n",
    "* captured image\n",
    "\n",
    "<img src=\"./assets/tut14_app_vid_cs_2.jpg\" width=\"400\">\n",
    "\n",
    "* <a href=\"https://ieeexplore.ieee.org/document/9064896\">Image source - ICCP 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/material-outlined/48/000000/video-trimming.png\" style=\"height:50px;display:inline\"> Application 4: Video Compressive Sensing\n",
    "--- \n",
    "\n",
    "* 64 frames recovered from 4 measurements (16 frames/capture)\n",
    "\n",
    "<img src=\"./assets/tut14_app_vid_cs_3.gif\" width=\"400\">\n",
    "\n",
    "* <a href=\"https://ieeexplore.ieee.org/document/9064896\">Image source - ICCP 2020</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/color/96/000000/code.png\" style=\"height:50px;display:inline\"> Available resources online\n",
    "---\n",
    "* https://github.com/computational-imaging/opticalCNN\n",
    "* https://github.com/vsitzmann/deepoptics\n",
    "* https://github.com/computational-imaging/DeepOpticsHDR\n",
    "* https://github.com/EliasNehme/DeepSTORM3D\n",
    "* https://github.com/computational-imaging/DepthFromDefocusWithLearnedOptics\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "* End-to-end Optimization of Optics and Image Processing - <a href=\"https://www.youtube.com/watch?v=iJdsxXOfqvw&t=266s&ab_channel=DingzeyuLi\">Vincent Sitzmann</a>\n",
    "* Neural Sensors - <a href=\"https://www.youtube.com/watch?v=tTYHoxA2RVg&ab_channel=StanfordComputationalImagingLab\">J.N.P Martel</a>\n",
    "* High Dynamic Range Imaging - <a href=\"https://www.youtube.com/watch?v=Pla8p9Nqlb8&ab_channel=StanfordComputationalImagingLab\">Christopher Metzler</a> \n",
    "* 3D Single Molecule Localization Microscopy - <a href=\"https://app.quantitativebioimaging.com/video/17\">Elias Nehme</a>  \n",
    "* Towards Neural Imaging & Signal Processing - <a href=\"https://www.youtube.com/watch?v=vTio0tuizHw&t=3984s&ab_channel=IEEESignalProcessingSociety\">Gordon Wetzstein</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "----\n",
    "* <a href=\"https://vsitzmann.github.io/deepoptics/\">End-to-end optimization of optics and image processing</a> - Vincent Sitzmann\n",
    "* <a href=\"https://dl.acm.org/doi/abs/10.1145/3388769.3407486\">ACM SIGGRAPH 2020 Courses</a> - Yifan (Evan) Peng, Ashok Veeraraghavan, Wolfgang Heidrich, Gordon Wetzstein\n",
    "* <a href=\"http://stanford.edu/class/ee367/\">Stanford EE367/CS448I </a> (Computational Imaging and Display) - Gordon Wetzstein\n",
    "* <a href=\"https://sites.google.com/view/sps-space/home?authuser=0\">IEEE SPACE Webinar</a> - IEEE Computational Imaging TC\n",
    "* Research papers:\n",
    "    * <a href=\"https://dl.acm.org/doi/10.1145/3197517.3201333\">End-to-end optimization of optics and image processing for achromatic extended depth of field and super-resolution imaging</a>\n",
    "    * <a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Chang_Deep_Optics_for_Monocular_Depth_Estimation_and_3D_Object_Detection_ICCV_2019_paper.pdf\">Deep Optics for Monocular Depth Estimation and 3D Object Detection</a>\n",
    "    * <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Metzler_Deep_Optics_for_Single-Shot_High-Dynamic-Range_Imaging_CVPR_2020_paper.pdf\">Deep Optics for Single-shot High-dynamic-range Imaging</a>\n",
    "    * <a href=\"https://ieeexplore.ieee.org/document/9064896\">Neural Sensors: Learning Pixel Exposures for HDR Imaging and Video Compressive Sensing With Programmable Sensors</a>\n",
    "    * <a href=\"https://arxiv.org/abs/1709.07223\">Convolutional neural networks that teach microscopes how to image</a>\n",
    "    * <a href=\"https://www.nature.com/articles/s41592-020-0853-5\">DeepSTORM3D: dense 3D localization microscopy and PSF design by deep learning</a>\n",
    "    * <a href=\"http://www.computationalimaging.org/publications/deepopticsdfd/\">Depth From Defocus With Learned Optics</a>\n",
    "    * etc.\n",
    "\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
