{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KE9w-ci502r"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
        "---\n",
        "#### <a href=\"https://taldatech.github.io/\">Tal Daniel</a> \n",
        "\n",
        "## Tutorial 03-04 - Convolution & Deep Learning\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_cnn.png?raw=1\" style=\"height:200px\">\n",
        "\n",
        "* <a href=\"https://mc.ai/how-does-convolutional-neural-network-work/\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERWlmNd9502w"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "\n",
        "* [2D Convolution](#-2D-Convolution)\n",
        "* [Convolution-based Classification](#-Convolutuion-as-Feature-Extractors-for-Classification)\n",
        "* [Convolutional Neural Networks (CNNs)](#-Convolutional-Neural-Networks-(CNNs))\n",
        "* [Regularization](#-Regularization---Preventing-Overfitting)\n",
        "* [Data Augmentation](#-Data-Augmentation)\n",
        "* [SVHN Classification with PyTorch](#-Building-a-CNN-Classifier-for-SVHN-with-PyTorch)\n",
        "* [The CNN Story](#-The-CNN-Story)\n",
        "* [Other Applications of CNNs](#-CNNs-Applications-in-Computer-Vision)\n",
        "* [The Problem with CNNs](#-Are-CNNs-the-Holy-Grail?-The-Problem-with-CNNs)\n",
        "* [Recommended Videos](#-Recommended-Videos)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aIb8jtLC5021"
      },
      "outputs": [],
      "source": [
        "# imports for the tutorial\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_CgT76R5024"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/layers.png\" style=\"height:50px;display:inline\"> 2D Convolution\n",
        "---\n",
        "\n",
        "Mathematically, 2D convolution is defined as: $$ f[n, m] * h[n,m] = \\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty} f[k, l]\\cdot h[n-k, m-l] $$\n",
        "\n",
        "Convolution is moving a window or filter across the image being studied. This moving window applies to a certain neighborhood of nodes as shown below – here, the filter applied is (0.5 × the node value):\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_xiv_filter.jpg?raw=1\" style=\"height:200px\" />\n",
        "\n",
        "* In our course, we will treat 2D convolution as *cross-correlation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-0yxLrP5028"
      },
      "source": [
        "### Numerical Example\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_num_ex.PNG?raw=1\" style=\"height:200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m62tG8Sq5029"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_anim.gif?raw=1\" style=\"height:400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E4rYc5e502-"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/color/96/000000/tweezers.png\" style=\"height:50px;display:inline\"> Convolutuion as Feature Extractors for Classification\n",
        "---\n",
        "\n",
        "* Convolution is useful since it helps us find interesting insights/features from images.\n",
        "* For example, the gradient/derivative filter helps us detect **edges** (low-level features).\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_derv_filt.PNG?raw=1\" style=\"height:150px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYWGJ3wz502_"
      },
      "source": [
        "* Recall that in classification tasks we need good features for better classification performance.\n",
        "* In *image classification*, we usually want to classify images into categories.\n",
        "* Imagine that we have a filter for each class, and that by applying this filter, we get a **probability** for the input image to be from this filter's class. <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_image_class.jpg?raw=1\" style=\"height:150px\">\n",
        "    * <a href=\"https://www.mathworks.com/solutions/deep-learning/convolutional-neural-network.html\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvXpgrTV503A"
      },
      "source": [
        "* What are features? Consider the following illustrative example - classifying *cats* and *dogs*.\n",
        "    * How do we tell the difference between cats and dogs? One can look at the length of the tail, shape of the paws, pattern of the fur and etc...\n",
        "    * So humans can usually tell these just by looking at the sample. But what do computers see?\n",
        "* In classification tasks, we need *good features* to learn a function that maps from samples to labels.\n",
        "* **Raw pixels** are usually not expressive enough features! That is because raw pixels do not capture the *spatial relationship* in the image.\n",
        "* Extracting features from raw pixels can be done using a deep learning network (which is a complex, non-linear function of the input).\n",
        "    * Using just linear layers (multi-layer Perceptron) might work for simple images (e.g. MNIST), but they have a lot of parameters! See tutorial 1.5 (Deep Learning and PyTorch basics) for more details.\n",
        "    * Using convolution, we can capture spatial structures (e.g., pixels the shape a tail)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOYD9lo9503E"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> Convolutional Neural Networks (CNNs)\n",
        "---\n",
        "\n",
        "* Convolutional Neural Networks (CNNs) are deep neural networks that contain layers of stacked convolution layers or filters. \n",
        "* They are mainy used for image datasets, but are also useful for other areas such as Natural Language Processing (NLP).\n",
        "* In the convolutional part of the CNN, we can imagine a moving filter sliding across all the available nodes / pixels in the input image. This operation can also be illustrated using standard neural network node diagrams: <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_xiv_layer.jpg?raw=1\" style=\"height:200px\" />\n",
        "* The first position of the moving filter connections is illustrated by the blue connections, and the second is shown with the green lines. The weights of each of these connections, as stated previously, is 0.5 (in this example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXumiIg503H"
      },
      "source": [
        "### CNNs Properties\n",
        "---\n",
        "\n",
        "### Feature mapping and multiple channels\n",
        "---\n",
        "* Since the weights of individual filters are held constant as they are applied over the input nodes, they can be trained to select certain features from the input data. \n",
        "* In the case of images, it may learn to recognize common geometrical objects such as lines, edges and other shapes which make up objects. \n",
        "* This is where the name *feature mapping* comes from. Because of this, **any convolution layer needs multiple filters which are trained to detect different features**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V__n_WXE503J"
      },
      "source": [
        "### Pooling\n",
        "---\n",
        "It is a sliding window type technique, but instead of applying weights, which can be trained, it applies a **statistical function** of some type over the contents of its window. The most common type of pooling is called **max pooling**, and it applies the $max()$ function over the contents of the window.\n",
        "There are two main benefits to pooling in CNN's:\n",
        "1. It reduces the number of parameters in your model by a process called *down-sampling*\n",
        "2. It makes feature detection more robust to object orientation and scale changes\n",
        "\n",
        "\n",
        "* Max-pooling can be seen as \"zoom-out\", allowing to detect bigger objects with smaller convolutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnZe1xVp503L"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_pooling.jpeg?raw=1\" style=\"height:200px\">\n",
        "\n",
        "* <a href=\"https://medium.com/@duanenielsen/deep-learning-cage-match-max-pooling-vs-convolutions-e42581387cb9\">Image Source </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buutbc43503M"
      },
      "source": [
        "* Other pooling operators:\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_other_pool.PNG?raw=1\" style=\"height:150px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4cg740i503M"
      },
      "source": [
        "* Pooling generalizes over lower level, more complex information. \n",
        "* Let’s imagine the case where we have convolutional filters that, during training, learn to detect the digit “9” in various orientations within the input images. \n",
        "* In order for the Convolutional Neural Network to learn to classify the appearance of “9” in the image correctly, it needs to in some way “activate” whenever a “9” is found anywhere in the image, no matter what the size or orientation the digit is (except for when it looks like “6”, that is). \n",
        "* Pooling can assist with this higher level, generalized feature selection. An example can be seen <a href=\"https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/\">here</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6mbB_Gn503N"
      },
      "source": [
        "### Strides and down-sampling\n",
        "---\n",
        "* In the pooling diagram below, you will notice that the pooling window shifts to the right each time by 2 places. \n",
        "* This is called a **stride of 2**, which should be considered both in the $x$ and $y$ direction. \n",
        "    * In other words, the stride is actually specified as $[2, 2]$. \n",
        "* One important thing to notice is that, if during pooling the stride is greater than 1, then the output size will be reduced. \n",
        "* As can be observed below, the 5 x 5 input is reduced to a 3 x 3 output. This is a good thing – it is called down-sampling, and it reduces the number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEfPhw9i503N"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_xiv_max_pool.jpg?raw=1\" style=\"height:200px\" />\n",
        "(images from adventuresinmachinelearning.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ax0oZgp503O"
      },
      "source": [
        "### Padding\n",
        "---\n",
        "* In the pooling diagram above there is an extra column and row added to the 5 x 5 input – this makes the effective size of the pooling space equal to 6 x 6. \n",
        "* This is to ensure that the 2 x 2 pooling window can operate correctly with a stride of [2, 2] and is called *padding*. \n",
        "* These nodes are basically **dummy nodes** – because the values of these dummy nodes is 0, they are basically invisible to the max pooling operation. \n",
        "* Padding will need to be considered when constructing our Convolutional Neural Network in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-7u6lsE503O"
      },
      "source": [
        "### The FC Layer\n",
        "---\n",
        "* The fully connected layer can be thought of as attaching a standard classifier onto the information-rich output of the network, to “interpret” the results and finally produce a classification result. \n",
        "    * That is, the output of the convolutuinal layers is the new \"input features\" for the classifier.\n",
        "* In order to attach this fully connected layer to the network, the dimensions of the output of the Convolutional Neural Network needs to be *flattened*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrGbV_y8503O"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_cnn.png?raw=1\" style=\"height:200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LWmwjR2503P"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/96/000000/layers.png\" style=\"height:50px;display:inline\"> Low Level (Shallow) and High Level (Deep) Features\n",
        "---\n",
        "* It is quite common to observe the features (outputs of the convolutional filters) at different levels of the network.\n",
        "* **Low Level** - also called shallow features (first layers), which include lines, corners and edges.\n",
        "* **Mid Level** - the middel level features, usually object parts.\n",
        "* **High Level** - also called deep features (final layers), which include whole objects (global)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWVKRSXF503P"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_low_high_level_feat.png?raw=1\" style=\"height:350px\">\n",
        "\n",
        "<a href=\"https://medium.com/analytics-vidhya/the-world-through-the-eyes-of-cnn-5a52c034dbeb\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbmjlp1K503P"
      },
      "source": [
        "### Calculating a convolutional layer output size\n",
        "---\n",
        "We define the following parameters of a *convolutional layer*:\n",
        "* $W_{in}$ - the width of the input\n",
        "* $F$ - filter size\n",
        "* $P$ - padding\n",
        "* $S$ - stride\n",
        "\n",
        "The output width:\n",
        "$$W_{out} = \\left \\lfloor \\frac{W_{in} - F + 2P}{S} + 1 \\right \\rfloor$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt4rH17O503Q"
      },
      "source": [
        "Consider the input images of size $28\\times 28$, filter size of $5\\times 5$, padding of 2 and a stride of 1, the output of the convolutional layers, just before the FC:\n",
        "$$W_{1, out} = \\frac{28 - 5 + 2*2}{1} + 1 = 28 \\rightarrow MaxPooling(2x2) \\rightarrow 28 / 2 = 14$$\n",
        "$$W_{2, out} = \\frac{14 - 5 + 2*2}{1} + 1 = 14 \\rightarrow MaxPooling(2x2) \\rightarrow 14 / 2 = 7$$\n",
        "So the input to the FC layer is $7x7=49$ (because we have 7 for the width and 7 for the height)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ2pHRrO503Q"
      },
      "source": [
        "### Non-Linear Activations\n",
        "---\n",
        "* The key change made to the Perceptron that brought upon the era of deep learning is the addition of **activation function** to the output of each neuron. \n",
        "* These allow the learning of non-linear functions. A (1-layer) neural network without an activation function is essentially just a linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phGOjsqn503Q"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_activation.PNG?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBfYZW87503Q"
      },
      "source": [
        "### Batch Normalization\n",
        "---\n",
        "* Batch normalization is a technique for improving the speed, performance, and stability of deep neural networks.\n",
        "    *  The reasons behind its effectiveness remain under discussion\n",
        "* It is used to normalize the input layer by adjusting and scaling the activations.\n",
        "* Formally:\n",
        "    * **Input**: $x \\in \\mathcal{R}^{N \\times D}$\n",
        "    * **Learnable Parameters**: $\\gamma, \\beta \\in \\mathcal{R}^{D} $\n",
        "    * **Intermediates**: $\\mu, \\sigma \\in  \\mathcal{R}^{D}, \\hat{x} \\in \\mathcal{R}^{N \\times D}$\n",
        "    * **Output**: $y \\in \\mathcal{R}^{N \\times D}$\n",
        "* In CNNs, we work with inputs of shape $[N, C, H ,W]$, where $N$ is the batch size, $C$ is the number of channels and $H, W$ are the height and width of the feature map respectively. BatchNorm in this case is performed **channel-wise**, i.e., on the channel dimension $C$ such that  $\\gamma, \\beta \\in \\mathcal{R}^{C} $."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvhE039Z503R"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_bn_algo.png?raw=1\" style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irq7pPJO503R"
      },
      "source": [
        "### CNN Vs. Fully Connected\n",
        "---\n",
        "* Fully connected networks with a few layers can only do so much – to get close to state-of-the-art results in image classification it is necessary to go deeper. \n",
        "    * In other words, lots more layers are required in the network. \n",
        "* However, by adding a lot of additional layers, we come across some problems. \n",
        "    * First, we can run into the vanishing gradient problem. However, this can be solved to an extent by using sensible activation functions, such as the ReLU family of activations or using residual blocks (ResNets). \n",
        "    * Another issue for deep fully connected networks is that the number of trainable parameters in the model (i.e. the weights) can grow rapidly. \n",
        "        * This means that the training slows down or becomes practically impossible, and also exposes the model to overfitting. CNNs try to solve this second problem by exploiting correlations between adjacent inputs in images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_lIvHnj503S"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/rope.png\" style=\"height:50px;display:inline\"> Regularization - Preventing Overfitting\n",
        "---\n",
        "* A common phenomenon in machine learning is that even though the training error keeps decreasing (training loss keeps going down, training accrucay goes up), the validation/test error goes down but then at some point it starts going up! (which is bad...)\n",
        "* This is called **overfitting**. Although it's often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to a testing set (or data they haven't seen before).\n",
        "* If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. \n",
        "    * We need to find a balance!\n",
        "* To prevent overfitting, the best solution is to use more complete training data. The dataset should cover the full range of inputs that the model is expected to handle. Additional data may only be useful if it covers new and interesting cases.\n",
        "* A model trained on more complete data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like **regularization**. \n",
        "* **Regularization** places constraints on the quantity and type of information your model can store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKMRNCCD503S"
      },
      "source": [
        "* The opposite of overfitting is **underfitting**. Underfitting occurs when there is still room for improvement on the test data. \n",
        "    * This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3SKg4XU503S"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_overfit.png?raw=1\" style=\"height:200px\">\n",
        "\n",
        "<a href=\"https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ9GeWd5503T"
      },
      "source": [
        "* Regularization usually comes in form of placing constraints on the parameters, or in the case of neural networks, constraints on the weights of the layers.\n",
        "* It introduces a cost term for bringing in more features with the objective function. Hence it tries to drive the coefficients of many variables to zero and hence reduce cost term.\n",
        "    * Common regularizations are $L_2, L_1$ regularizations: $$ \\text{New Loss}_{L_2} = \\text{Original Loss} + \\lambda \\mid \\mid w \\mid \\mid^2$$\n",
        "* For deep neural networks (and CNNs) a common regularization technique is **Dropout**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbUbDPGM503U"
      },
      "source": [
        "#### Dropout Regularization\n",
        "----\n",
        "* First presented in <a href=\"http://jmlr.org/papers/v15/srivastava14a.html\">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>, 2014.\n",
        "* Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.\n",
        "* During training, some number of layer outputs (i.e. neurons) are randomly ignored or “dropped out” with some probability $p$. This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer.\n",
        "* Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically **take on more or less responsibility** for the inputs.\n",
        "* Dropout is activated **only during training** (`model.train()`). In test time, it is turned off (`model.eval()`).\n",
        "\n",
        "Read more - <a href=\"https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\">A Gentle Introduction to Dropout for Regularizing Deep Neural Networks</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgXGqqIx503U"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_dropout.png?raw=1\" style=\"height:250px\">\n",
        "\n",
        "<a href=\"https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVOZ_PFS503V"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/variation.png\" style=\"height:50px;display:inline\"> Data Augmentation\n",
        "---\n",
        "* Data augmentation is a common technique to improve results and avoid overfitting.\n",
        "* How do we get more data when there is limited number of samples? We can perform data augmentation.\n",
        "* Data augemnetation enriches the dataset by adding variations of the original samples.\n",
        "    * And as you know, deep learning flourishes when there is A LOT of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXqFPXKB503V"
      },
      "source": [
        "* Popular augementation techniques:\n",
        "    * **Flip** - Flip images horizontally and/or vertically.\n",
        "    * **Rotation** - Rotate the images at certain degrees. This may change the size of the image, thus, cropping or padding is a common fix for that.\n",
        "    * **Scaling** - The image can be scaled outward or inward. This may also change the size of the image, thus, resizing (also stretching) is often followed.\n",
        "    * **Cropping** - Randomly sample a section from the original image. Then resize this section to the original image size. This is called Random Crop.\n",
        "    * **Translation** - Move the image along the X or Y direction (or both). This forces the neural network to look everywhere.\n",
        "    * **Noise** - Over-fitting usually happens when the network tries to learn high frequency features (patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially has data points in all frequencies, effectively distorting the high frequency features. This also means that lower frequency components (usually, your intended data) are also distorted, but your neural network can learn to look past that. Adding just the right amount of noise can enhance the learning capability (e.g., add Salt and Pepper).\n",
        "    \n",
        "Read More - <a href=\"https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/\">Data Augmentation | How to use Deep Learning when you have Limited Data</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMiujCxQ503X"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_augment.png?raw=1\" style=\"height:300px\">\n",
        "\n",
        "<a href=\"https://towardsdatascience.com/machinex-image-data-augmentation-using-keras-b459ef87cd22\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0KhDnir503X"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/dog.png\" style=\"height:50px;display:inline\"> The SVHN Dataset\n",
        "---\n",
        "\n",
        "* The SVHN (The **S**treet **V**iew **H**ouse **N**umbers) dataset consists of 600,000 32x32 colour images of the 10 digits. There are 73257 digits for training, 26032 digits for testing, and 531131 additional, somewhat less difficult samples, to use as extra training data.\n",
        "* SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates and order of magnitude more labeled data and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. \n",
        "* <a href=\"http://ufldl.stanford.edu/housenumbers/\">Official Site</a>\n",
        "\n",
        "<img src=\"http://ufldl.stanford.edu/housenumbers/32x32eg.png\" style=\"height:300px;\">\n",
        "\n",
        "* In the homework assignment you will work with CIFAR-10, which is a classification dataset for 10 classes of objects.\n",
        "  * Considered a hard dataset - low res images, not even humans can easily distinguish the classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0bueIz8503Y"
      },
      "source": [
        "# ------------------------- <img src=\"https://img.icons8.com/color/96/000000/code.png\" style=\"height:50px;display:inline\"> CODE TIME -------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "1e1121aa4a29452ca7a88b5f1f27ed3b",
            "77dcea1a4a174d219482b74a509de3e7",
            "a8b132d58a1e48948df23cc22f1c867c",
            "70701e55d8984afa9c1c1376e014c689",
            "cc23d5c3467d483b96da9a88baed8602",
            "186fb06fa4204213a5de85ed2666066a",
            "bb5a12d52cbf4f6d8ed78c38754365b2",
            "5a4d985baa1847078afe9baa70e2c98c",
            "10bf30af32fe4c6a82548571b01f6028",
            "3116e2147ef94f49b10d0800ca6b5d2d",
            "e3e99de7fd614eb58a92a9be9bdeae42",
            "33c7d8ba7b5541e581963b26e3b991d7",
            "c260b9b44a124024b10e6dc19aac4095",
            "60193def73744ef1a5fa5df79ad661e5",
            "cf8b0497d9de411a9e3dcc7135735ebd",
            "3441d0e1288642ac9d4e4c637dfe7d3b",
            "5cdae4b1da054079930b9b33af4e69b7",
            "0b8425cab9334503a1721c45d98b60a3",
            "a430d750de334df9970ee8f10c677178",
            "22a8e9ed622b42f29b14f3b4506c8e89",
            "f84c8bbd08534b5799f8854b868f7b9a",
            "6b3a1c5cff7e472f982e1ca179dd0c85"
          ]
        },
        "id": "zXp4FzF5503a",
        "outputId": "8c2de1c5-c50a-4531-e80c-ba393dbab42b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./datasets/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e1121aa4a29452ca7a88b5f1f27ed3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./datasets/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c7d8ba7b5541e581963b26e3b991d7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# define pre-processing steps on the images\n",
        "# also called \"data augementation\" (only done for the train set)\n",
        "\n",
        "# define transforms for the trainset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ColorJitter(brightness=63. / 255., saturation=[0.5, 1.5], contrast=[0.2, 1.8]),\n",
        "    transforms.ToTensor(),  # (almost) Always use this!\n",
        "    # SVHN dataset's:     Mean - R    Mean - G   Mean - B       STD - R     STD - G     STD - B\n",
        "    transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614)),\n",
        "])\n",
        "\n",
        "# define transforms for the trainset\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614)),\n",
        "])\n",
        "\n",
        "# load dataset\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./datasets', split='train', download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./datasets', split='test', download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "HzyuYlG9503b",
        "outputId": "e0ea6bdf-39c4-48ec-99e1-442bbd6b5298"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x180 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqUAAACpCAYAAAAShURgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dTagtXXrXn1VVe+/zce990x3bKCoZSMCPCAoZiBAiigTamQoqqHHgJKKgMwcB0SA4ECcRR4IfODLQA/FjFAkxiCQGETXREJVASwe7ydv3nnvO2R9VtRzcm75n/Z/n3av2PvvcOuee3w+aflftVatW1V7rqXX3+T//lXLOBgAAAAAwJ83cHQAAAAAAYFEKAAAAALPDohQAAAAAZodFKQAAAADMDotSAAAAAJgdFqUAAAAAMDssSgEAAABgdliUHkhK6a+klP5TSmmTUvonc/cHYCoppbfyvyGl9BNz9wugRkrpn6eUvpFSepNS+uWU0l+au08AU0kp/ZmU0i+llK5TSv8rpfSDc/fpsZIwzz+MlNKfMLPRzH7YzM5zzn9x3h4BHE5K6YWZ/ZqZfTXn/DNz9wdgHyml32tmv5Jz3qSUfpeZ/bSZ/fGc8y/M2zOA/aSU/piZ/SMz+9Nm9nNm9lvNzHLO/3fOfj1Wurk78NTIOX/NzCyl9ANm9ttn7g7AsfxJM/t/Zvbv5+4IQI2c83+/W3z/v99pZixK4bHzt8zsb+ec/+P7MovRPfDne4DnyY+Y2T/L/KkEnggppX+YUroxs/9hZt8ws38zc5cA9pJSas3sB8zsKymlX0kpfT2l9A9SSudz9+2xwqIU4JmRUvpeM/shM/unc/cFYCo5579sZi/N7AfN7Gtmtpm3RwBVvsfMFmb2p+zduP39ZvYHzOzH5uzUY4ZFKcDz48+b2c/mnP/P3B0BOISc85Bz/ll7J5360bn7A1Dh9v3//0TO+Rs552+Z2d83s6/O2KdHDYtSgOfHXzB+JYWnTWfvNKUAj5ac8+dm9nV7p4H+zuGZuvMkYFF6ICmlLqV0ZmatmbUppbOUEglj8CRIKf0hM/ttZvaTc/cFYAoppd/83lLnRUqpTSn9sJn9WTP7qbn7BjCBf2xmf/X9OP6Smf11M/tXM/fp0cKi9HB+zN79JP83zOzPvf9v9CHwVPgRM/tazvlq7o4ATCTbuz/Vf93MPjezv2dmfy3n/C9n7RXANH7czH7ezH7ZzH7JzP6zmf2dWXv0iMGnFAAAAABmh19KAQAAAGB2WJQCAAAAwOywKAUAAACA2WFRCgAAAACzc5CV0dmqy5cXqw8H0oSTTpBH5ZqQ5KxRysdcMulJE+4tSaXkzgka0UMPkmd2/0bfvL2tVxKGwb6Vc/7KvS9+YlJKubnnP79evnxZuciERrIW3YHglP113OfRZbXKEfPF354cmTB/ao8o6QQKTmhcnXTQ5xGff/76UY5bM7Omuf/YhU+XxxpzzU4Td+HTZRzjsXvQovTyYmVf/SO/+8MBGXFR+M+j70mBtKHV351SHu37srztd+Xneo13FwqO3flUTwmq66GuacvP5Wk2esDc7brHcRSujXH/x9FR6ddP/cx/Obgbn3/bfvXgkz4CTWN2eXm/Nn7oD//BvZ+nYLxk/R6GcuU2yJefh8G1MeTyWK9tyDn6D7So3WGQfo31xXGTdXbL3B9lcRg8jyQPSReQbVvOp27hI8pysSjrnC2L8qIr29A2zcykq/aT/+JfP8pxa/buOX72au5ewGPl1z9/nDHX7H3cvZi7F/BYuXobj13+HQMAAAAAs8OiFAAAAABm537bY+qfHycISEZZB9f+NG9mtt7Kn+flz4+7vt/b5jvK63bS10b77v+S6uUJbXlkqY9z6Z+Hdm0rffd/z4+eqf75Naiyp/475G+0zRRR5PNlzMGAuEMbfKxPXf9cr396H3QsmFk/6J/vy/KuL8t58H97H4Zy/oyDzEHV2Oif8y3QXAtZ/3xv/s/mbVs2snB/rtdw5NtIXfnn+3Yn8oVWx7Ef++kJ/Vv8/Pzcfs/v+77vlJ2EaLm0w9FWDpc7jeOUmHtYP47RH+opLo6H57iz6udUqxze+Z/7+f9w8DkAnzpPJzoDAAAAwCcLi1IAAAAAmB0WpQAAAAAwO/fTlCoTtJxao9+WR65uvT/mm6uboqwa0j7Q4rleiCio7bq9n3eBx4/qUDdSXp2dFeXlBJ2VagT9Kfu1jDGiq2u8INDdXaAjhA+ojVIrT3Dw8kePOi9JOfoKBmcBJR6jo35e6kfDNsRCbXCTMhgvzjNNr1GWo8cx9qUedOjKNp06svNzsBm20saqrCDPowu63bSn8GH7OOScbbyjsx81Tk2Rg1Zk6lGYqmko9ZxjJKU6po6TpYou9YgmfJv+UK1rU1S69VYAgF9KAQAAAGB2WJQCAAAAwOywKAUAAACA2WFRCgAAAACzc9pEp4CxorLv5fObdZnIYGZ2/fa6KK/d3t2HC8hVyN+pkfOERKeFGHmr8f+m84/Xm07L53rClPwjzWuSj0NDaU0QePCR8MSRTJ4sbvJtCvZYn+OffNF3LUlIuk9C1oSTKEtJk7I0K0vm5BClOqVybicZdIOM3BzM65yl85LElWQjgCYH/XhCOX3ZzPo7MaORkO3jqwVBZH9m03HG98ewP7FpkvF9JbHpmLwvPdJEE7eW+KVthCc8r9+AfserS/ubf/T7v1Nu3Tv1/uOuDyazJkI3bfmeTm2ZUqlJrGZmo2Shuhp6wG3aYdakslLXlIFYNwvJjY9VfnMUKUsc3vR+/bSVe+mHsq+7UV4QQcbtoivP+bs//V9dnVPxvGYJAAAAADxKWJQCAAAAwOywKAUAAACA2fn4SsKaNicwwt+K+EjN8kfRAEXaJKe92pZtqBIjVgSVRy/Oy89ViZGCfgxq/K96DhGIxnKv8kqNnON0V6GmVIpLRKX7GAbRO4peKAV6oFa+lyyaKidDigbdQqqIIHTQcuDiP4qWaRjLczrRELkh+e6kophF69Q39U0ekohVdX60tv/zd23ILKtsWhD1Ks0i9j2OnLNt78Sq5VI0+b0PEH6+79fuRZrS2oYkD6FDjaXvlXs5yjx/f57DEZJST7DxA5yeaMMR1V1rkFAJ6XrwjfSiw9zuyhWCe9cHgWghsftcyiruX6nY38ysU/1n+bFqSG/WfvOUt9L3jbTRSyy37Oe+5tw8JMwcAAAAAJgdFqUAAAAAMDssSgEAAABgdg4TEiaTZWxdu+RWvaoPFQFbH+gZxlzz1ZuwtlYpkniIaptjoNXqnE5ov/6z32xcG9uKT6ljgnZrrN5/9LncL/882UvuS63OKB61eQz0QOqPJ7pLPSOUR8r3klJ5VtuWZ0Vav5xlrIuGSv1A1aPPzCzLfBi2ci+7clyr96mZ13K2y7Lvei9t6welnpOcD2kjn/s2npBNqeWcC32nxqBxPFwL7mJdMGZqdWqa0ymoXtTHV7POeT2X5QnWphUF6bQa9z/jGZLM7I6Ovq46P5xl8nF3K3EjJ9Vlijf61o/lq+ubonzbl+/yfid5HYFP6bIrY9PLy/J98JnUTyvXhHv/76Qfb67XRfn1rV9zvJX7U9XpaJJfs/VttMNDfHsxzCUAAAAAmB0WpQAAAAAwOyxKAQAAAGB2jjCnPO06ttE9g6M950Uj0nuhatnGFKHRCaheJZAZehVuTZ30MPcyfqRn9KkwiBAniRa6bf1U8ioc0UzKVxCpdloRQCaZH1nHTyCY1H3qcyf7J+u+9oF+aJCOOLmnSDvHne9Isyj72jblM1uIfrALYoHqThvx8WuSesO6Jp4Woikdx+Weyr9R5zBf0kgf6r2gT68prelFzcyaRmP7MfkEqp+veJuOQV6D6xi+zgfjfDjruQ6K190HbUj46iUUqYb06rbUj5qZffPN26K82ZU6y+2g8T/QlMo42/VnZYVGYqpdujaSBPSr2/JF9PptqSl9s/M+pRvJB1BPcrVp3QS5MOpL/ZCwMgEAAACA2WFRCgAAAACzw6IUAAAAAGaHRSkAAAAAzM7HV2vLMrgRg9mLCy/k7/P+RKatGHf75CGr6ac9U5brei8HXyRARfn1y7pKXpQfXadyDhSozlt9viNvYcnrsdRoEo6I4xs/9rNYHSfNfHIZRoF5vojlx0H6IZlOY2BI3SSpI5+Pkk0wLP0DacTovpOkJdmPwNqlfx5NKkNWanSDAqkfZBtGmwM8VrKVSUZTjO81Ph5jhK/H+r523XoEqfXrC87a24YLbpPaPD2Y6X8Bdx5EJ8lAOfyqDntyUdzdybdxsynLr6+vi/K3vl0mNZmZbQfZLETiTisvgGijD02gut1tpUYZh1bJp3GNubzBb1+Vbb6VxKZ1kH+YJKHw7LJMuMqSCHsTJH5ttyQ6AQAAAMAzgkUpAAAAAMwOi1IAAAAAmJ2PrilV/9yl6B3Ozy/8Oc7sWNbSN7dFcb2LzI9Fl1pdjweCF2fcLNo0p2cLcO75dQ1prWc1A/5pKiv+fbKP5LRsYvyeRBBpZk7vKZqqJJ83FgiCxv2G+24MtoEuScZYFtNm/XxIXj+UpW9Zq8hlm8hbXbvWlf1IoqladF4P2i3kAbS6+Ya//+fGFN3pXaLNRtTYfhy3Uj5c21r7PKrvdahaQwbahDacKlX0gWN/uD5WLxs/j2qznxTJzNpAn37/Vj/QLvx8H27LMXG9K9cHb29Lw/lNIExtl6uifH5WlhdyX7veG87fbOS6omV9LX1fnfnlmI6jKxGNXm0ldgebuLyUvn/51cuirHrxqzdvXBu7zf03y5jKM5smAAAAAPAYYVEKAAAAALPDohQAAAAAZucITek9feBEAqea0ubCd6kTL9NxLNfSm77UOzWBprSq55nibyd1tM3VUnSGS38vK9EIbtaHe/eJrZg7Z+s0Ub7NUf3/qld93rSiVWzFEFM9SM3MukbGdtrvjzkE03Fs92t59LJjYP7XqBDV1Sk/z04waqZjSP3zBpNzUtAP1dBKlVY1t5E+ULRc+r0kp+2N9LFPyJU353t7bx6q7TxFmx+rH8dwjIZ0QqtSnuQw/Qw4raZ0kPZyEHe3EpuuRMt5dV1qPTeD/65eXrwoyi9eleVWFjI3Vz5OtxLve+nqRsyvb9ZBrBJP5SvRdr7ZlX1fBUOsXZTvlUvRmKqkdrXweT3rRj1WH47nOEsAAAAA4JHBohQAAAAAZodFKQAAAADMzkf3Ka1LTLy+oxOtkZa99WfgU3qCzYmdhvSs1K+9kD1lk26QHrDbln31+1IHetCsGtKyjeam3A9X9/F9d539ZVB04JblyPfWjcssekjRJalONbyuFtUbd/Rt6HjJqocNfEmV2m7xjeqcgzoqM03qQyra1zbwXHXHtCwXyTkIOB9vG+f7k1IReJzH5gl0mFEbGofq14n8Qfdf52NpSJWjNKSNvipr3q+HX+KTI1kxLFwMmfCM9NWlHss7TbAws43klNyuy/fhzbb0FM2t95jWPJalfKGD6EH73msu81D2rWuWZRsSm9a9X7fo+/9GNKQufaaL3vVlpWRlX5fibarrq/dHg2MPA1MHAAAAAGaHRSkAAAAAzA6LUgAAAACYHRalAAAAADA7h6lXs5X67klLWqdU3vt51KTT3VZ8iiMJuvqF1/KtIq2vmi6rILiVjQCWq2AjADX7vTwviqMIqDWJycxsFPX37bqsk8ebsv5tKfQ2M9uS2XRSoqfpcuvETLmWPGRm1orh/KhnneR71NkQDP68t+iJkonUxF+u02ry2OhnqZrjt5LYpJfVjTbMzMa8f0OCx4TkijxIslDURiexzCdgSnLlhG6cpu8nyFg98ApmZo0mz7rEJ3fGqbrzCTPBWF8zKOWc3r/a7HZbRgHxm3cxofH5lNZJrNZvc7MpDfivr65N2W3V2F4SijpN0gw2ApB3/W6UCNeU8bALkmWHQdcUZaLTuWzyswoStFM/5W11Gpg5AAAAADA7LEoBAAAAYHZYlAIAAADA7NzPEXWKnM0te1XcOWFdXLuOM9MPdBXSyKEa0xjtmOpjg447XZXUka5H/vsq71hKpXaKVgtJ6UEMg2rKymI+QtupEqJIhukkVaIxNdEop8BMWkeDzgVXIxTI1ursN8I/ikDrpfMliSCsEbVrDozSt8MTcs9XHb8QPuWK5t5VnxAvPobxfajjrxjwH8NDqFJ1+kfdjE3JP23uRgV958YPfv8mJSoAdRpLM7u+LmO1pmUkE1168L20EleXstHHW3Gtv7kpDfnN/JhYLsu+n52VZvqrhQ94VzelVjXvyvtt5Rw1wn9H2fdBTP1174DVyq+G2oymFAAAAACeESxKAQAAAGB2WJQCAAAAwOzcT1PqtBh1XV2txhjU6J1HXlkenW4s8NmSvqoG0GkCI02Q6F/VcnSSZEiu43ruvFBF8GFm/W2pX7m5XRflzbY0b1OPwXfX3d8PKOnF202f2LjyeqA8+u+uRLU7Xrczqua6Ud2y1NdBaWY2lNM8iU9nK5ftg7CQ8k7K0qbT1PkR1TiRqMxJtWB1nsYR2mZ5bzvznqT9LjA3fMQ89C8HkU4zihn7mFLf61IPusTJeIjL6r10UTLAMwuyyZK1d572UMsvMbNcGRRJ53ugKe13Zazeig5To2ybfOzWsJslOLVtec7FxWXQ25LLi1JDenEpMTSI/0sJ5xqHrS/btBwI8SVW931ZZyNy2GEb6EcDD9WHgl9KAQAAAGB2WJQCAAAAwOywKAUAAACA2bmfpvQoxzc5Z4L9ldc/ajlLOaKmVVEfxmB/7AdYwns9l2pCAo2taGjXYsTWi4eaanLfX7ko5eh+4TsMqikSV1HVHL0/S8riqam6peC7dvs0awUnfqoL1/JY9mOUfkZDod6sao4CXbepJnv/2G+CcVuTNqm2MQcdH0JH2EdKsnKg6DM5wh9XfZwjPah+MzX9Z9jGEed49jcyybd0ionogVSbfGb60Snom22IYoQ8N7U7HuRdv1UTUvM5FYPmoEibffbxYLct293clHvd9+J13HU+f6AVb9Mvvbwoyufn5fJrjDxXL0vN6NV1qZftJb4NwfMYRYZ6fau63FJUeht4Ow+RafQDwS+lAAAAADA7LEoBAAAAYHZYlAIAAADA7LAoBQAAAIDZOTjR6a4E9pgVrXpqq+g4bFUr1bT+oXBfxP3yuU9TOfzupmnb9yd3aNfXay9+vl2LQfi6VDK7xCZE9ydnSqqM1unUHD7JgShXSidMzcPYTygbRQw/irA/q7l09mL57bass+vLctNIsuEYdLQtjyUx+m+bVj73yQOaHNFpwpmeEI39CcmVj4nijifMZZdzU8lHjSKdu4yLub2UoyS9shwayh9ILbEpTrg67Lph7abbX0enzFIrPEeyHfzykQeryZG6yc1mJ87vZjaMGpv2Z6FtN7oxitnNjfRjkMQnSSgaBn+frUvkLOnEgN86HzMvV2dFebUoO9ZLUlfeScfN7PZ2VR4Yy/vdrMrBulkHmxoEyWAPBb+UAgAAAMDssCgFAAAAgNlhUQoAAAAAs3Mv83xvIDxF8KRm4Kr1nNCGXrfSr7CSalcmabWkr6pNO0LKqd1SY/zbSO9yuy7K602pb9mq+e0ETVVNqvjcSTJA2iP+PZflIeszT4EGqXHm70dcVzRWqstSjekwRPqh8pymkTpSbJM3W27E+T5JnVE0pU1XN2x2c+yJ6UWr5P3++GNglu3me2X+R5rLzrWxtwnbNtHmG4eN3dhMv9ZGWV5GulXRg+ruEBpzY11qeazrSh3ecllewz0/Q9p/HPrUJGb0PlZF+s67ZPk8tT5orDelWb6JpnSQUzZb34+cS0389VWp91zIWD0L4t1Kxu7FooyhvWpqm/qbfBjK69zclH3fRFO5/XgrBH4pBQAAAIDZYVEKAAAAALPDohQAAAAAZudgTWnzhYUjcfrQeqOq+Rl71btNaaNWo95GU/Ehm9LGVvQb602p71jfek3pRrxLVRM1DXlmKJ72csxQT05jXddI1pE21eo00FQmPTjs15CqBtXMbKc+dSKobkRzFHn0teLL1y7L8rKdMOdEM9XIMx1S3U/v6emnP3znGvv6QFPaiFat0zAv8THSP7o6lRnQ9P5z7ZuzT5ZyFMe8lnNvN63pAoNQfWZyme22jLGRTlffKc3F/ldnk4LPnT780+fuXNPQFEVDn6ZRPnedu6NL7DAbxp3UkSslXS/4iKC61KERP1Dp127r39O9+D+36tvblrkhzYvSk9TMbNmVHqNfevlSGhEf10BT3YlWNaVS6/r2VnzOg3nYLbxn9EPBL6UAAAAAMDssSgEAAABgdliUAgAAAMDsHKYpTXb4MrYqo5ngZaca0qof6EOttStmp1oO7l0P9eKzthOR6UZFp2a2Fb2L6m6mobqa56d3OgzR5ZygRfUgjbaLP5zAt892Uu73fj7mQFOXVWMleikJJbqts5lZI5pR1ZiqyCyd5CEH3q+Bh+pTwenpI0/NseL9rG0cuDe8WaCxjJ6zHqsL+adcWdqUYu+1fV7Lur8f4afuOuoNXZZV1zvlup86Op2nWAr7p5ilFJhqqiZeNKPqMZ3Uc9nMkgSf84vzoqxWqJuNb2Mt7/abXRlnF9elpvRMBdNm9tln5XWXq+8qyqtNqfXchEkFZbxTyfRaclQ2wTvkbHUvS/uD4JdSAAAAAJgdFqUAAAAAMDssSgEAAABgdliUAgAAAMDsfDz16hfgpd91MbgzeldT4lBQftj6O66tTs3724y6oUkHm12pOl5vSqH+Nkh08u1qv7RC9DzKY8tAmA8fSI0KyEshfJTEkHflOaO4JydJSMlRstnByT5epJ7FYHqU+ZLVGD8YLymVbSTpWNIkpqUfTws12JdzGkkuCG3wRx3r+83yNWHBzCwdkdgzH7kYW948v244r3FKPw6TpaTsHrtpopN/lWg/Ri27MBUFzCmx/QNT9hHxpv5TEpA0uXT/Bi5afnfd553oNAWfDOV21ykJNjpwkaORJFVpwyVcmtnleWla/91fLhOMJIfJhsHHme3rN2V5KN/tulHOzc7348vdRdmvyxdFedGX51xvymQqM7Ptruzbr9/eFOVesra64HmcX6zcsYfiKUVnAAAAAPhEYVEKAAAAALPDohQAAAAAZucITenddewxGpnKOSpeOryFsMYUpeq+opmXkNZ66kyrzUuitms1yy8NdbeD18w5DZTriOq96v1QY3NQ9PmI7iYwLXbaTdF25hOYw49iJh2rh8sxlMetlNVMPzDPl76qplB1WWpQbWbWNnpO+blqPZ2M14INB5xe1l3VN5Jml9IfQLljyRTzfLcdiepQ5fvtxuB5OFHp/l6G5vnVzUUiPaBUOdToPqpfMfqfZPJfyx8wfcaBpnTC/T4rjtrzpZzgQ6Shl7JGAD0j2qTj7KzUUH726mVR7ofyKte3t66Nz19rbJYNF+T+1/3GtTGI3l9j5llb9vM2MPHP0tebt6XudLsrz7k48/rRyzPM8wEAAADgGcGiFAAAAABmh0UpAAAAAMzOEUKBw3Sk3nfs8Oacbkx1VVK/mXBbkd7zcMo1vbPdCywUB/FVu9mIhnRbnjQGPmwTXAX9hQX1JT0/W1bPec6oFW5uVKfpn59qSE384FQgFKgf3TeZ3QySfuStKXkQ7zq5GdUt5VBTV9ZJtijKTh/aBUKtVo9V1V6uidF5u9ba9MLUSP/4qLnTXdUlet1mEB/d/Yr+MfIpdbL1w59ZTe85RR+7XO6P5d5Ptd5P/f67rrxGGD0P1aWCmSXLd56L+oPGD3r/u0vnexuND7lOLxr61JaxunVxyayVCdAtyrjSrcpzzla+H20r724Z3xrLh+DeN2KIOkhM3Iov6W2kbX1T9mOnvqTdWVG+vCz1s2Zmr17gUwoAAAAAzwgWpQAAAAAwOyxKAQAAAGB2DtOUZiskH06JMWGJmyr6x0jftB3E30v2kHX60EjfU9kOXr0ax/BmZL9z+XTQ/Y8DjcjNbakhvb0pvcl0H1rV0MXo/tDiQxg0cbEodTWvLs8nXOf5on6gshV8rIRSIaqgbUztSVkUDbI36nRefr32S7xwU468/9Rjdf+9Oamnee9S9c+0CV65agfr7lcvHHrBPiGf0lTqFaft017iz5kg7lM5nNSZpKGsaUh1D/p6E/6A6EEjj1Hta+1pNMu6vr5pOilXTznqu3v63JmAKioN/LO9T3lZR9cPbfLflUavXjT1C9F/qj4+uKz7fhddqTGNpK1tVzayEQ/RRsb/MPhBJBaitlF/1F25Frq68ZrS69tyjZHa8l1/sSz1oi8C/ejl2cdbH/BLKQAAAADMDotSAAAAAJgdFqUAAAAAMDssSgEAAABgdu6l+NeUiij3oWaWP0X6rWbeQ7/fYL4PFfOnWH/vN/HvXT98R3Z9KUzuXeLGhCQEr/73de7QBZlOrRzD/Hk/edCRLCbumoHzcB2R64ohc5CkNErfh0EF9vVkKR0empOU2lE+9+MpST5BEp/7JoVZSfeiiTOunixTZn4tkUdrPFT+Tc0cf1LiT7WOxrF6k6eIddrEtBafYaLTngcTTXcXefRB6xiaFHY1kWnKt1XWSWPZ2UYunKKdclxOanmObnMSNbGRxcx6XZ612ZRt7oLFj99ORZLH2rKNZevbGAe/ic9DwUoEAAAAAGaHRSkAAAAAzA6LUgAAAACYnXu6SIuuLqxTMZR3xQm6G+d2vPeS0+tUEfN80fOtd2VZ9aNmZut1aeTbb1WroV9J9O+G/cqypehFlwtvMHx2VupsFpH7L3wgl99bziJMHMvPzczGXD73sdEZoiIiL7JqRKu6E83xIC7nobRJdKaDbNCwE61TE5n+i7bL9VQFooFgrJE6WvZiz0ANqc06s/xKP80sHbdrwXw0d/+zrgV3h1SWpydM0HbWfr2Ije81uM+lqdzf+2M2Aqg+j95r8Mbx4+nyHicTNOOqVa9UaIM4o5t0uNHpwq4flyKzdJFJQ0gUUvxGQfvj2xiMQ+3qelOuKW43pTH+NlhzSAqONbWwG9zM7W7tjj0U/FIKAAAAALPDohQAAAAAZodFKQAAAADMzr2EhMcohI5TL4mOyokg9DamiEor1/qyLC4AAAfTSURBVAh1RqIBqehh+8gzTPR8WsX739WdCPWc5aI8sDpTnzazs1Wpd9RzQFGdzRRdoiqC1Jizdg2zQb9r1XvKeFJP37BZHXORhlRI2ve2HFOtfN464ZJZEv1nUuPSowhEtAWBR+8JrvrxSEUsWnZlrGs6H8KbRuo4q8e6X6ge03jYySmhlk10lbU2o5jbdaqH15sRnWYTPY/KAS+ydW34YbTfDTbSlM6nqZ2JZJbSPd8rIhltJYYsgvGvHslJ4tsoL90+6GOSC6vsNLXiWxpGlbJvScZm10rM7OqRSdcUW9GYbnb+HZLlHWHiQ7rrS13qN1/7mNpOet+dBlYiAAAAADA7LEoBAAAAYHZYlAIAAADA7BysKVXX0RrVVa/qiuoX9fvY18pBu6r/VM1IF+ibVBO1U92QfN7rxvbm9Sxe7+V2xHVtaM90a/uleI6eLfzXvFqq7gyf0v2ID6mzx4s8CNW71BnCFaVIHan70OuISlm0bDnQRw37Z+E4QfeVUzk+WttfTq3XRzlf0lb1stKvYB7XqWlMA5++R0yycr47HaZO/gAnZezrmtKDfUnr0vcqUzxXvT70/nHLvRum9LvygPi15zTkipRxEYx/d0zMm8dBtf2Rf2pZx+V+uDDjg0rbaIwsb0b1sJGfuNqwqnb7RnxKozwWF/Dkuhu5l23gp9t8RDk0cwcAAAAAZodFKQAAAADMDotSAAAAAJgdFqUAAAAAMDsnzW6ZpA8XEbJLZgiEuk6IL2V3E9FS26nXVbk8xVC6TFxRPfB6V34eG0rLZa1WDpIQRO2viUyrZWlK3gVi8HhzAPhCIi18wRRzYTG6FxV/lGChxva7XA4gNctvJmVplML3Jqlq35vaJxHLJ0la8gk3kTG+nHOEH7OeM0gimBr0m7s3nzzwmEnJrLuTmNCpef6E3xWqZvlhrJOyxEe9bhQvFT3HGfIHRuid2whA6jRT+lFLpt3/Loiq1D7XRFrw+UTjhPnv5qrEzGjMnK9WRbm1MhlocNf1I0Lf07uNJJwOZVzxbZr5l0Z5zkLM8ldtEJjyIEVJ2tpKPzS31sxaSTpVo/9eN08JcnabIIH2oWBlAgAAAACzw6IUAAAAAGaHRSkAAAAAzM69NKXHqWb2r4MjreNKNCKXlxdFuRVdyRDqUkuhxHan4gvRNwWG4t1S9UylkKSpGOObmXXdfm2W08sGjeix1VlpunsmJryLzpvyNgnz/IMQ/2E3PEJDdnWDV+HRfvN4M7OdukeLRHKUzyPr+Cw7QyTV8mXVf3ptU5NUD6g1ynMibasa/SepM8hcyNkLpAZ5pvpIm0FN/F0TE/TBj4iUCl2lM8+foA13McbJ+L2ITDWRuunHlJ8zan1TPWCkD1Stsmtzyv0f+tNL+GLTg6JlDTfPuGc/njzJysmmWvZgc5nKqkJll6vOT+bLVRnPdKOYG9nUZghi1c261KG+ub0pyl0rba59/ojeykKC0Zkzz/fBaimHNmt5ZpqjMmFR1upLopWBGWlb63uSnIxnN00AAAAA4PHBohQAAAAAZodFKQAAAADMzomFhHWPOPWR652G0usfX7w4K+tIr9e35QH1bjQz60UTtd2pBkT0fYGm1HmsipnZYllqMTZb/3h7OafflP2YIplaigbkTPzOFkv1/6v7lPKvk/0E8qd7o3pQ1X6+P3pgq3XxjyqXBieyPGY0tHuL05rVvgeaM/XUU5wvqe/IUxrryZLMX523EzyIa2bIARqnaj6kk7StFV/SZaR9r2potVx/Bx2bDVG0oP7Arp/RWU9p5N2fZKU3qb5S81jXrus3leTIauGf6fm55FSsRMva7/f+NDO7vl0X5devS03pQvSfb2+8/j2Lt/PLi07KZa7Mi1WwHBNf6rX6lO5knvY+ZrYyrVqJ962ckw5+55yW5zVLAAAAAOBRwqIUAAAAAGaHRSkAAAAAzM5BmtLXVzf2b//dLzxUX+BoXs/dgU+e//Y/f3HuLsAzJKX9HsKqczeL9OJSHtUb+fj+/Qahx2hFD+p8StUH2vy9O7dQPaUPnkfliHcgDXyu3RH5XDSmy+B7qTuZfop80FUm1a4HP4mpzrRRX2Kpvwg0pepLumj270Fv2evO+22Z6/F2XWpMl2OpB90433OzhehjX4mG9NVFmStzeVZ+bma2VV9SyUPod+pjHYyysfRtTdKGTqFl8lrf5eLjmTvzSykAAAAAzA6LUgAAAACYHRalAAAAADA7LEoBAAAAYHZObJ4PAAAnI5Xm+ephP8U8P6pzlyhJqdamJvacJNEp2DhFc7KOQpOO+hNkdgnuXoNEpy5IwvrkaT4kFeVwRw2trwc08ams0I7e6P3lxUVR/p7v/lJZIZeJwVfXupGO2SAm9eurq7LCeZlQdBFs/PDyRdmP3/KbXhTlTh9H9klKmj61SmLIfyZzaBNsFiIm/itJZPpM+vldl5eujUUniU6/+L9dnVPxDGcJAAAAADw2WJQCAAAAwOywKAUAAACA2Uk5e03GF1ZO6Ztm9qsP1x144nxvzvkrc3dCYdxChUc5bs0Yu1CFsQtPlXDsHrQoBQAAAAB4CPjzPQAAAADMDotSAAAAAJgdFqUAAAAAMDssSgEAAABgdliUAgAAAMDssCgFAAAAgNlhUQoAAAAAs8OiFAAAAABmh0UpAAAAAMzO/wfvxqDWDHk+AgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# let's see some of the images\n",
        "def convert_to_imshow_format(image):\n",
        "    # first convert back to [0,1] range from [-1,1] range - approximately... (In the previous step we have shifted the mean to 0)\n",
        "    image = image / (image.abs().max() * 2) + 0.5\n",
        "    image = image.numpy()\n",
        "    # convert from CHW to HWC (C - Channels, H - Height, W - Width)\n",
        "    # from 3x32x32 to 32x32x3\n",
        "    return image.transpose(1,2,0)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True)\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "fig, axes = plt.subplots(1, len(images), figsize=(12,2.5))\n",
        "for idx, image in enumerate(images):\n",
        "    axes[idx].imshow(convert_to_imshow_format(image))\n",
        "    axes[idx].set_title(labels[idx].data.item())\n",
        "    axes[idx].set_xticks([])\n",
        "    axes[idx].set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4yIFgDr503c"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/50/000000/fire-element.png\" style=\"height:50px;display:inline\"> Building a CNN-Classifier for SVHN with PyTorch\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_cifar_arch.PNG?raw=1\" style=\"height:200px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jn0Xcqwx503c"
      },
      "outputs": [],
      "source": [
        "class SvhnCNN(nn.Module):\n",
        "    \"\"\"CNN for the SVHN Datset\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(SvhnCNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
        "            # What are the dims after this layer? \n",
        "            # How many weights?\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),  # <- Why is this here?\n",
        "\n",
        "            # Modified Conv Layer block 3\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # As we go deeper - use more channels!\n",
        "        )\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(8192, 512),  # <- How do we know it's 8192? Why 512 later?\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 10)  # <- Why 10 here?\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7x22Cb503d",
        "outputId": "ceb7c5c1-8403-45ba-ded4-cdffaa5533cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 8, 8])\n",
            "torch.Size([1, 8192])\n",
            "num trainable weights:  4482058\n"
          ]
        }
      ],
      "source": [
        "# calculating the output size of the convolutional layers, before the FC layers\n",
        "dummy_input = torch.zeros([1, 3, 32, 32])\n",
        "dummy_model = SvhnCNN()\n",
        "dummy_output = dummy_model.conv_layer(dummy_input)\n",
        "print(dummy_output.shape)\n",
        "dummy_output = dummy_output.view(dummy_output.size(0), -1)\n",
        "print(dummy_output.shape)\n",
        "\n",
        "# calculating the number of trainable weights\n",
        "num_trainable_params = sum([p.numel() for p in dummy_model.parameters() if p.requires_grad])\n",
        "print(\"num trainable weights: \", num_trainable_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqWiznql503e"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/flat_round/64/000000/presentation.png\" style=\"height:50px;display:inline\"> Training the CNN Model\n",
        "---\n",
        "* So we have the model, but how do we train it to output the correct class of the input image?\n",
        "* As you have probably noticed, the output of the final fully-connected layer is a vector of length 10, which is exactly the number of classes we have!\n",
        "* We mentioned that we want entry $i$ of the final vector to be the probability of the input to be from class $i$.\n",
        "* But how do we force this vector to output probability and not just some numbers?\n",
        "* We consider the final output vector to represent *scores*, which we will normalize to be probabilities using the **Softmax** function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFBMxNC503e"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/s.png\" style=\"height:50px;display:inline\"> The Softmax Function\n",
        "---\n",
        "* The Softmax function is defined as: $$ Softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^M e^{x_j}}, i \\in [1,...,M], x \\in \\mathcal{R}^M  $$\n",
        "* This forces the output vector to sum to 1, just like probabilities.\n",
        "* <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_softmax.PNG?raw=1\" style=\"height:150px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdX0_KUk503f"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/plasticine/100/000000/unicorn.png\"  style=\"height:50px;display:inline\"> Making Predictions\n",
        "---\n",
        "* OK great, we have an output vector of probabilities, so how we predict the label of the input image?\n",
        "* Simple! Just take the $argmax$: $$ \\hat{y} = Softmax(CNN(x)) $$ $$ c_{pred} = argmax_i (\\hat{y})  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8LNwNa2503f"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/bearish.png\" style=\"height:50px;display:inline\"> Loss Function - Cross Entropy \n",
        "---\n",
        "* In order to train the model in an end-to-end fashion, we need to define a loss function which we can minimize using optimization techniques.\n",
        "* Let us assume that our model output (after softmax) is $\\hat{y}$ and the real label (the real class, given to us) is $y$.\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_x_y.PNG?raw=1\" style=\"height:150px\">\n",
        "* **Insight**: $$ argmax(\\hat{y}) = argmax(y) $$\n",
        "* Ideally, this is what we would want from our model, so what loss function would drive $\\hat{y}$ to be as close as possible to $y$?\n",
        "* As $y$ is a *one-hot vector* and $\\hat{y}$ represents probabilities, the **Cross Entropy** loss function fits right in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRHQEdeB503g"
      },
      "source": [
        "* Let $W$ denote the weights of the CNN, and $W^{*}$ the optimal weights.\n",
        "* In this case, the optimal weights are: $$ W^{*} \\leftarrow argmin_{W} \\left(-\\sum_{x, y} 1 \\cdot \\log (p_c) \\right) $$ $$ p_c = \\hat{y}_{c} $$\n",
        "* $c \\in [1,...,M]$ is the correct class, $\\hat{y}_{c}$ is the $c^{th}$ entry in the output vector $\\hat{y}$.\n",
        "* So the **Cross Entropy** loss function is: $$\\mathcal{L} = -\\log (p_c)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPCoiu5Z503h"
      },
      "source": [
        "Let's analyze this loss fucntion, which represents how bad we are currently doing:\n",
        "$$ p_c = 0 \\rightarrow \\mathcal{L} = -\\log (0) = \\infty $$\n",
        "$$ p_c = 0.1 \\rightarrow \\mathcal{L} = -\\log (0.1) = 2.3 $$\n",
        "$$ p_c = 0.9 \\rightarrow \\mathcal{L} = -\\log (0.9) = 0.1 $$\n",
        "$$ p_c = 1 \\rightarrow \\mathcal{L} = -\\log (1) = 0 $$\n",
        "\n",
        "**The larger the loss, the worse the prediction**. We want to minimize it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jON9oZn2503h"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/nolan/64/compress.png\" style=\"height:50px;display:inline\"> Minimizing the Loss Function with Gradient Descent\n",
        "---\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_gd.gif?raw=1\" style=\"height:200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oRe-hDI503i"
      },
      "source": [
        "* In order to perform Gradient Descent, we need to calculate the dervatives with respect to the network's weights.\n",
        "* Due to memory reasons, when the dataset is large, we cannot compute the gradients on the whole dataset, and thus we train in **mini-batches** of the data. The `batch_size` is a *hyper-parameter* which needs to be tuned. Usually the sizes are 32, 64, 128...\n",
        "* In order to propagate the gradients through all of the layers, we need to use the chain rule when calculating the gradients. This is called **backpropagation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS1aSuaX503i"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/plasticine/100/000000/serial-tasks.png\" style=\"height:50px;display:inline\"> Backpropagation\n",
        "---\n",
        "* Denote the output of the $k^{th}$ layer as $f(Z^{(k)}) $ and the input to the next layer $Z^{(k+1)}$.\n",
        "* **Forward Pass**: $Z^{(k+1)} = f(Z^{(k)}) $\n",
        "* **Backward Pass**: $\\delta^{(k+1)} = \\frac{\\partial E}{\\partial Z^{(k+1)}}$\n",
        "* Applying the **chain rule** for a single layer: $$ \\frac{\\partial E}{\\partial Z^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial Z^{(k+1)}}{\\partial Z^{(k)}} = \\delta^{(k+1)}\\frac{\\partial f(Z^{(k)})}{\\partial Z^{(k)}} $$\n",
        "* The **gradient with respect to layer parameters** (if it has any): $$ \\frac{\\partial E}{\\partial W^{(k)}} = \\frac{\\partial E}{\\partial Z^{(k+1)}} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}} = \\delta^{(k+1)} \\frac{\\partial Z^{(k+1)}}{\\partial W^{(k)}}  $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFVNnj4C503k"
      },
      "source": [
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_backprop.jpg?raw=1\" style=\"height:400px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao8iezbw503k"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/64/000000/discount.png\" style=\"height:50px;display:inline\"> Learning Rate\n",
        "---\n",
        "* As we use Gradient Descent, we also have the **learning-rate** *hyper-parameter* which needs to be tuned.\n",
        "* <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_lr.PNG?raw=1\" style=\"height:250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRtDhS-G503k"
      },
      "source": [
        "# ------------------------- <img src=\"https://img.icons8.com/color/96/000000/code.png\" style=\"height:50px;display:inline\"> CODE TIME -------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cBkZgwD503l",
        "outputId": "cc8df93c-080e-4e1c-8911-feca9919b124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "# time to train our model\n",
        "# hyper-parameters\n",
        "batch_size = 128\n",
        "learning_rate = 1e-4\n",
        "epochs = 20\n",
        "\n",
        "# dataloaders - creating batches and shuffling the data\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# device - cpu or gpu?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device}')\n",
        "\n",
        "# loss criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# build our model and send it to the device\n",
        "model = SvhnCNN().to(device) # no need for parameters as we alredy defined them in the class\n",
        "\n",
        "# optimizer - SGD, Adam, RMSProp...\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yc16wnTV503n"
      },
      "outputs": [],
      "source": [
        "# function to calcualte accuracy of the model\n",
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    confusion_matrix = np.zeros([10,10], int)\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            for i, l in enumerate(labels):\n",
        "                confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EssJgGD5503p",
        "outputId": "fc946584-12f8-4153-b4c7-3d79f8791abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 1.5085 | Training accuracy: 67.998% | Test accuracy: 79.049% | Epoch Time: 705.57 secs\n",
            "Epoch: 2 | Loss: 0.7483 | Training accuracy: 81.628% | Test accuracy: 85.356% | Epoch Time: 755.45 secs\n",
            "Epoch: 3 | Loss: 0.5565 | Training accuracy: 84.997% | Test accuracy: 87.642% | Epoch Time: 730.38 secs\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()  # put in training mode\n",
        "    running_loss = 0.0\n",
        "    epoch_time = time.time()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # send them to device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)  # forward pass\n",
        "        loss = criterion(outputs, labels)  # calculate the loss\n",
        "        # always the same 3 steps\n",
        "        optimizer.zero_grad()  # zero the parameter gradients\n",
        "        loss.backward()  # backpropagation\n",
        "        optimizer.step()  # update parameters\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.data.item()\n",
        "\n",
        "    # Normalizing the loss by the total number of train batches\n",
        "    running_loss /= len(trainloader)\n",
        "\n",
        "    # Calculate training/test set accuracy of the existing model\n",
        "    train_accuracy, _ = calculate_accuracy(model, trainloader, device)\n",
        "    test_accuracy, _ = calculate_accuracy(model, testloader, device)\n",
        "\n",
        "    log = \"Epoch: {} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "    epoch_time = time.time() - epoch_time\n",
        "    log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "    print(log)\n",
        "    \n",
        "    # save model\n",
        "    if epoch % 20 == 0:\n",
        "        print('==> Saving model ...')\n",
        "        state = {\n",
        "            'net': model.state_dict(),\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoints'):\n",
        "            os.mkdir('checkpoints')\n",
        "        torch.save(state, './checkpoints/svhn_cnn_ckpt.pth')\n",
        "\n",
        "print('==> Finished Training ...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2kIjoHG503q"
      },
      "outputs": [],
      "source": [
        "# load model, calculate accuracy and confusion matrix\n",
        "model = SvhnCNN().to(device)\n",
        "state = torch.load('./checkpoints/svhn_cnn_ckpt.pth', map_location=device)\n",
        "model.load_state_dict(state['net'])\n",
        "\n",
        "test_accuracy, confusion_matrix = calculate_accuracy(model, testloader, device)\n",
        "print(\"test accuracy: {:.3f}%\".format(test_accuracy))\n",
        "\n",
        "# plot confusion matrix\n",
        "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
        "ax.matshow(confusion_matrix, aspect='auto', vmin=0, vmax=1000, cmap=plt.get_cmap('Blues'))\n",
        "plt.ylabel('Actual Category')\n",
        "plt.yticks(range(10))\n",
        "plt.xlabel('Predicted Category')\n",
        "plt.xticks(range(10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NHqK3f51503r"
      },
      "outputs": [],
      "source": [
        "# visualize filters - more methods in the appendix to this tutorial\n",
        "# observe available layers, in our case, the stacked layers are called \"conv_layer\"\n",
        "print(model.conv_layer)\n",
        "# extracting the model features at the particular layer number\n",
        "layer = model.conv_layer[0]  # to plot other layers, see the appendix tutorial\n",
        "# get the weights\n",
        "weight_tensor = layer.weight.data.cpu()\n",
        "\n",
        "# get the number of kernals\n",
        "num_kernels = weight_tensor.shape[0]    \n",
        "\n",
        "#define number of columns for subplots\n",
        "num_cols = 12\n",
        "# rows = num of kernels\n",
        "num_rows = num_kernels\n",
        "\n",
        "#set the figure size\n",
        "fig = plt.figure(figsize=(num_cols, num_rows))\n",
        "\n",
        "# looping through all the kernels\n",
        "for i in range(weight_tensor.shape[0]):\n",
        "    ax1 = fig.add_subplot(num_rows, num_cols, i+1)\n",
        "\n",
        "    #for each kernel, we convert the tensor to numpy \n",
        "    npimg = np.array(weight_tensor[i].numpy(), np.float32)\n",
        "    #standardize the numpy image\n",
        "    npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "    npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "    npimg = npimg.transpose((1, 2, 0))\n",
        "    ax1.imshow(npimg)\n",
        "    ax1.axis('off')\n",
        "    ax1.set_title(str(i))\n",
        "    ax1.set_xticklabels([])\n",
        "    ax1.set_yticklabels([])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLNtzW_X503s"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/50/000000/book-shelf.png\" style=\"height:50px;display:inline\"> The CNN Story\n",
        "---\n",
        "\n",
        "* **1996** - Lenet-5 - core of CNR check reading system, used by US banks.\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_lenet5.gif?raw=1\" style=\"height:200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htAtxwEy503s"
      },
      "source": [
        "* **2012** - ILSVRC - Imagenet Large Scale Visual Recognition Challenge\n",
        "    * Imagenet data base: 14M labeled images, 20K categories.\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_imagenet.PNG?raw=1\" style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0_IBp5q503s"
      },
      "source": [
        "* **2012** - AlexNet wins the challenge by a significant margin!\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_ilv_12.PNG?raw=1\" style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVWbU_El503t"
      },
      "source": [
        "* **2013** - thanks to deep CNNs, the results only keep improving.\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_ilv_13.PNG?raw=1\" style=\"height:300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcfxrH3y503t"
      },
      "source": [
        "* **2 Years ago**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_ilv_20.PNG?raw=1\" style=\"height:300px\">\n",
        "* **Today** - <a href=\"https://paperswithcode.com/sota/image-classification-on-imagenet\">Link</a>\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_ilv_22.PNG?raw=1\" style=\"height:350px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SoXlgXm503t"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/lightning-bolt.png\" style=\"height:50px;display:inline\"> CNNs Applications in Computer Vision\n",
        "---\n",
        "* **Object Detection**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_obj_det.PNG?raw=1\" style=\"height:200px\">\n",
        "    * <a href=\"https://medium.com/better-programming/real-time-object-detection-on-gpus-in-10-minutes-6e8c9b857bb3\"> Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vSPhVGr503u"
      },
      "source": [
        "* **Semantic Segmentation**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_segnet.png?raw=1\" style=\"height:200px\">\n",
        "    * <a href=\"https://missinglink.ai/guides/computer-vision/image-segmentation-deep-learning-methods-applications/\">Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSxpUxaM503u"
      },
      "source": [
        "* **Super Resolution**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_superres.PNG?raw=1\" style=\"height:300px\">\n",
        "    * <a href=\"https://arxiv.org/pdf/1609.04802.pdf\">Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kla51RoV503u"
      },
      "source": [
        "* **Style Transfer**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_style_trans.jpeg?raw=1\" style=\"height:300px\">\n",
        "    * <a href=\"https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee\">Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy1r7VAN503u"
      },
      "source": [
        "* **Image Editing**\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_image_edit.jpg?raw=1\" style=\"height:300px\">\n",
        "    * <a href=\"http://people.csail.mit.edu/junyanz/projects/gvm/\">Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DAtGzla503v"
      },
      "source": [
        "* **Image Generation**\n",
        "    * StyleGAN V2 - <a href=\"https://thispersondoesnotexist.com/\">thispersondoesnotexist.com</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNKVawVN503v"
      },
      "source": [
        "* **Multi-Signals**\n",
        "    * Synthesizing Obama: Learning Lip Sync from Audio\n",
        "    * <img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_obama.png?raw=1\" style=\"height:300px\">\n",
        "    * <a href=\"http://grail.cs.washington.edu/projects/AudioToObama/\">Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFl6WBfF503w"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/cute-clipart/64/000000/easter-egg.png\" style=\"height:50px;display:inline\"> Are CNNs the Holy Grail? The Problem with CNNs\n",
        "---\n",
        "#### Deep NNs are sensitive to adversarial attacks.\n",
        "* For example: consider the following image, where on the left, we have an image of a pig that is correctly classified by a state-of-the-art convolutional neural network. \n",
        "* After perturbing the image slightly (every pixel is in the range [0, 1] and changed by at most 0.005), the network now returns class “airliner” with high confidence.\n",
        "    \n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_pig.png?raw=1\" style=\"height:200px\">\n",
        "\n",
        "<a href=\"http://gradientscience.org/intro_adversarial/\">Image Source</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6x9k3qp503w"
      },
      "source": [
        "#### Recognition algorithms generalize poorly to new environments\n",
        "\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_cow.PNG?raw=1\" style=\"height:200px\">\n",
        "\n",
        "<a href=\"https://arxiv.org/pdf/1807.04975.pdf\">Recognition in Terra Incognita (Beery et al., 2018)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31U4ceKo503x"
      },
      "source": [
        "#### Neural Networks tend to exhibit undesirable biases\n",
        "<img src=\"https://github.com/AlexYoro/EE046746_Computer_Vision_Tutorial/blob/winter23/assets/tut_conv_fairness.PNG?raw=1\" style=\"height:300px\">\n",
        "\n",
        "* The reasons why the model learns these biases are unclear. \n",
        "    * One hypothesis is that despite the balanced distribution of races in pictures labeled basketball, black persons are more represented in this class in comparison to the other classes\n",
        "\n",
        "<a href=\"https://arxiv.org/abs/1711.11443\">ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases (Stock and Cisse, 2018)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcRMiilT503x"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
        "---\n",
        "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
        "* These videos do not replace the lectures and tutorials.\n",
        "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
        "\n",
        "#### Video By Subject\n",
        "* Convolutional Neural Networks -  <a href=\"https://www.youtube.com/watch?v=iaSUYvmCekI\"> Convolutional Neural Networks | MIT 6.S191 </a>\n",
        "    * A previous version of this lecture - <a href=\"https://www.youtube.com/watch?v=H-HVZJ7kGI0\"> Convolutional Neural Networks | MIT 6.S191 </a>\n",
        "* Deep Neural Networks with PyTorch - <a href=\"https://www.youtube.com/watch?v=_H3aw6wkCv0&t\"> Stefan Otte: Deep Neural Networks with PyTorch | PyData Berlin 2018 </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eACgYlv3503y"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* EE 046746 Spring 21 - <a href=\"https://taldatech.github.io/\">Tal Daniel</a> \n",
        "* Some slides from CS131 and CS231n (Stanford)\n",
        "* Deep Learning with Pytorch on CIFAR10 Dataset - <a href=\"https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/\">Zhenye's Blog</a>\n",
        "* CIFAR-10 Classifier Using CNN in PyTorch - <a href=\"https://www.stefanfiott.com/machine-learning/cifar-10-classifier-using-cnn-in-pytorch/\">Stefan Fiott</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    },
    "rise": {
      "scroll": true
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e1121aa4a29452ca7a88b5f1f27ed3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77dcea1a4a174d219482b74a509de3e7",
              "IPY_MODEL_a8b132d58a1e48948df23cc22f1c867c",
              "IPY_MODEL_70701e55d8984afa9c1c1376e014c689"
            ],
            "layout": "IPY_MODEL_cc23d5c3467d483b96da9a88baed8602"
          }
        },
        "77dcea1a4a174d219482b74a509de3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186fb06fa4204213a5de85ed2666066a",
            "placeholder": "​",
            "style": "IPY_MODEL_bb5a12d52cbf4f6d8ed78c38754365b2",
            "value": "100%"
          }
        },
        "a8b132d58a1e48948df23cc22f1c867c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a4d985baa1847078afe9baa70e2c98c",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10bf30af32fe4c6a82548571b01f6028",
            "value": 182040794
          }
        },
        "70701e55d8984afa9c1c1376e014c689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3116e2147ef94f49b10d0800ca6b5d2d",
            "placeholder": "​",
            "style": "IPY_MODEL_e3e99de7fd614eb58a92a9be9bdeae42",
            "value": " 182040794/182040794 [00:08&lt;00:00, 38701323.58it/s]"
          }
        },
        "cc23d5c3467d483b96da9a88baed8602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186fb06fa4204213a5de85ed2666066a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb5a12d52cbf4f6d8ed78c38754365b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a4d985baa1847078afe9baa70e2c98c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bf30af32fe4c6a82548571b01f6028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3116e2147ef94f49b10d0800ca6b5d2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e99de7fd614eb58a92a9be9bdeae42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c7d8ba7b5541e581963b26e3b991d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c260b9b44a124024b10e6dc19aac4095",
              "IPY_MODEL_60193def73744ef1a5fa5df79ad661e5",
              "IPY_MODEL_cf8b0497d9de411a9e3dcc7135735ebd"
            ],
            "layout": "IPY_MODEL_3441d0e1288642ac9d4e4c637dfe7d3b"
          }
        },
        "c260b9b44a124024b10e6dc19aac4095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cdae4b1da054079930b9b33af4e69b7",
            "placeholder": "​",
            "style": "IPY_MODEL_0b8425cab9334503a1721c45d98b60a3",
            "value": "100%"
          }
        },
        "60193def73744ef1a5fa5df79ad661e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a430d750de334df9970ee8f10c677178",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22a8e9ed622b42f29b14f3b4506c8e89",
            "value": 64275384
          }
        },
        "cf8b0497d9de411a9e3dcc7135735ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f84c8bbd08534b5799f8854b868f7b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b3a1c5cff7e472f982e1ca179dd0c85",
            "value": " 64275384/64275384 [00:05&lt;00:00, 36249891.87it/s]"
          }
        },
        "3441d0e1288642ac9d4e4c637dfe7d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cdae4b1da054079930b9b33af4e69b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8425cab9334503a1721c45d98b60a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a430d750de334df9970ee8f10c677178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a8e9ed622b42f29b14f3b4506c8e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f84c8bbd08534b5799f8854b868f7b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b3a1c5cff7e472f982e1ca179dd0c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}